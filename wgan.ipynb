{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch import optim\n",
    "import tensorboard\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, fd):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Conv2d(channels_img, fd, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(fd, fd * 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(fd * 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(fd * 2, fd * 4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(fd * 4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(fd * 4, fd * 8, 4, 2, 1),\n",
    "            nn.BatchNorm2d(fd * 8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(fd * 8, 1, 4, 2, 0),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, fg):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(channels_noise, fg * 16, 4, 1, 0),\n",
    "            nn.BatchNorm2d(fg * 16),\n",
    "            nn.ReLU(),  \n",
    "            nn.ConvTranspose2d(fg * 16, fg * 8, 4, 2, 1),\n",
    "            nn.BatchNorm2d(fg * 8),\n",
    "            nn.ReLU(),  \n",
    "            nn.ConvTranspose2d(fg * 8, fg * 4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(fg * 4),\n",
    "            nn.ReLU(),  \n",
    "            nn.ConvTranspose2d(fg * 4, fg * 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(fg * 2),\n",
    "            nn.ReLU(),  \n",
    "            nn.ConvTranspose2d(fg * 2, channels_img, 4, 2, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if isinstance(m,nn.BatchNorm2d):\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * alpha + fake * (1 - alpha)\n",
    "\n",
    "    # critic scores\n",
    "    mixed_scores = critic(interpolated_images)\n",
    "\n",
    "    # gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5] Batch 100/2035                   Loss D: -0.5362, loss G: -0.0119\n",
      "Epoch [0/5] Batch 200/2035                   Loss D: 0.5708, loss G: -0.0386\n",
      "Epoch [0/5] Batch 300/2035                   Loss D: 0.2384, loss G: -0.2373\n",
      "Epoch [0/5] Batch 400/2035                   Loss D: 0.6278, loss G: -0.3796\n",
      "Epoch [0/5] Batch 500/2035                   Loss D: -0.1947, loss G: -0.2303\n",
      "Epoch [0/5] Batch 600/2035                   Loss D: -0.2974, loss G: -0.1531\n",
      "Epoch [0/5] Batch 700/2035                   Loss D: -0.5889, loss G: -0.1627\n",
      "Epoch [0/5] Batch 800/2035                   Loss D: 0.2336, loss G: -0.1702\n",
      "Epoch [0/5] Batch 900/2035                   Loss D: -0.4959, loss G: -0.4040\n",
      "Epoch [0/5] Batch 1000/2035                   Loss D: -0.6435, loss G: -0.1689\n",
      "Epoch [0/5] Batch 1100/2035                   Loss D: 0.1258, loss G: -0.8791\n",
      "Epoch [0/5] Batch 1200/2035                   Loss D: -0.6311, loss G: -0.2651\n",
      "Epoch [0/5] Batch 1300/2035                   Loss D: -0.4515, loss G: -0.1832\n",
      "Epoch [0/5] Batch 1400/2035                   Loss D: -0.3232, loss G: -0.4708\n",
      "Epoch [0/5] Batch 1500/2035                   Loss D: -0.6209, loss G: -0.2096\n",
      "Epoch [0/5] Batch 1600/2035                   Loss D: -0.5783, loss G: -0.3149\n",
      "Epoch [0/5] Batch 1700/2035                   Loss D: -0.6582, loss G: -0.2041\n",
      "Epoch [0/5] Batch 1800/2035                   Loss D: -0.5365, loss G: -0.2134\n",
      "Epoch [0/5] Batch 1900/2035                   Loss D: -0.5949, loss G: -0.0859\n",
      "Epoch [0/5] Batch 2000/2035                   Loss D: -0.7599, loss G: -0.0837\n",
      "Epoch [1/5] Batch 100/2035                   Loss D: -0.3246, loss G: -0.4448\n",
      "Epoch [1/5] Batch 200/2035                   Loss D: -0.6810, loss G: -0.1356\n",
      "Epoch [1/5] Batch 300/2035                   Loss D: -0.6788, loss G: -0.1439\n",
      "Epoch [1/5] Batch 400/2035                   Loss D: -0.2352, loss G: -0.3789\n",
      "Epoch [1/5] Batch 500/2035                   Loss D: -0.6744, loss G: -0.0463\n",
      "Epoch [1/5] Batch 600/2035                   Loss D: -0.5847, loss G: -0.3159\n",
      "Epoch [1/5] Batch 700/2035                   Loss D: -0.8323, loss G: -0.0697\n",
      "Epoch [1/5] Batch 800/2035                   Loss D: -0.6642, loss G: -0.1773\n",
      "Epoch [1/5] Batch 900/2035                   Loss D: -0.7941, loss G: -0.0906\n",
      "Epoch [1/5] Batch 1000/2035                   Loss D: -0.5467, loss G: -0.1657\n",
      "Epoch [1/5] Batch 1100/2035                   Loss D: -0.6288, loss G: -0.2299\n",
      "Epoch [1/5] Batch 1200/2035                   Loss D: -0.7347, loss G: -0.1319\n",
      "Epoch [1/5] Batch 1300/2035                   Loss D: -0.7288, loss G: -0.1043\n",
      "Epoch [1/5] Batch 1400/2035                   Loss D: -0.7478, loss G: -0.1668\n",
      "Epoch [1/5] Batch 1500/2035                   Loss D: -0.5454, loss G: -0.3519\n",
      "Epoch [1/5] Batch 1600/2035                   Loss D: -0.6328, loss G: -0.2419\n",
      "Epoch [1/5] Batch 1700/2035                   Loss D: -0.8837, loss G: -0.0688\n",
      "Epoch [1/5] Batch 1800/2035                   Loss D: -0.6236, loss G: -0.1610\n",
      "Epoch [1/5] Batch 1900/2035                   Loss D: -0.8096, loss G: -0.0720\n",
      "Epoch [1/5] Batch 2000/2035                   Loss D: -0.6218, loss G: -0.2281\n",
      "Epoch [2/5] Batch 100/2035                   Loss D: -0.8542, loss G: -0.0781\n",
      "Epoch [2/5] Batch 200/2035                   Loss D: -0.8226, loss G: -0.0474\n",
      "Epoch [2/5] Batch 300/2035                   Loss D: -0.7125, loss G: -0.0646\n",
      "Epoch [2/5] Batch 400/2035                   Loss D: -0.8875, loss G: -0.0473\n",
      "Epoch [2/5] Batch 500/2035                   Loss D: -0.6994, loss G: -0.1050\n",
      "Epoch [2/5] Batch 600/2035                   Loss D: -0.7545, loss G: -0.1242\n",
      "Epoch [2/5] Batch 700/2035                   Loss D: -0.8305, loss G: -0.1322\n",
      "Epoch [2/5] Batch 800/2035                   Loss D: -0.4578, loss G: -0.4498\n",
      "Epoch [2/5] Batch 900/2035                   Loss D: -0.5672, loss G: -0.2683\n",
      "Epoch [2/5] Batch 1000/2035                   Loss D: -0.8065, loss G: -0.0437\n",
      "Epoch [2/5] Batch 1100/2035                   Loss D: -0.8523, loss G: -0.0915\n",
      "Epoch [2/5] Batch 1200/2035                   Loss D: -0.7290, loss G: -0.2419\n",
      "Epoch [2/5] Batch 1300/2035                   Loss D: -0.4365, loss G: -0.3140\n",
      "Epoch [2/5] Batch 1400/2035                   Loss D: -0.6394, loss G: -0.2606\n",
      "Epoch [2/5] Batch 1500/2035                   Loss D: -0.8239, loss G: -0.0558\n",
      "Epoch [2/5] Batch 1600/2035                   Loss D: -0.0998, loss G: -0.4447\n",
      "Epoch [2/5] Batch 1700/2035                   Loss D: -0.7480, loss G: -0.1864\n",
      "Epoch [2/5] Batch 1800/2035                   Loss D: -0.5471, loss G: -0.3877\n",
      "Epoch [2/5] Batch 1900/2035                   Loss D: -0.9183, loss G: -0.0378\n",
      "Epoch [2/5] Batch 2000/2035                   Loss D: -0.7578, loss G: -0.1480\n",
      "Epoch [3/5] Batch 100/2035                   Loss D: -0.8242, loss G: -0.0939\n",
      "Epoch [3/5] Batch 200/2035                   Loss D: -0.8177, loss G: -0.0764\n",
      "Epoch [3/5] Batch 300/2035                   Loss D: -0.6135, loss G: -0.1303\n",
      "Epoch [3/5] Batch 400/2035                   Loss D: 0.1805, loss G: -0.3706\n",
      "Epoch [3/5] Batch 500/2035                   Loss D: -0.7974, loss G: -0.0840\n",
      "Epoch [3/5] Batch 600/2035                   Loss D: -0.7241, loss G: -0.1131\n",
      "Epoch [3/5] Batch 700/2035                   Loss D: -0.6564, loss G: -0.2502\n",
      "Epoch [3/5] Batch 800/2035                   Loss D: -0.4767, loss G: -0.4264\n",
      "Epoch [3/5] Batch 900/2035                   Loss D: -0.6213, loss G: -0.2985\n",
      "Epoch [3/5] Batch 1000/2035                   Loss D: -0.8234, loss G: -0.1059\n",
      "Epoch [3/5] Batch 1100/2035                   Loss D: -0.6495, loss G: -0.1492\n",
      "Epoch [3/5] Batch 1200/2035                   Loss D: -0.6516, loss G: -0.1537\n",
      "Epoch [3/5] Batch 1300/2035                   Loss D: -0.7731, loss G: -0.1577\n",
      "Epoch [3/5] Batch 1400/2035                   Loss D: -0.8016, loss G: -0.1264\n",
      "Epoch [3/5] Batch 1500/2035                   Loss D: -0.7719, loss G: -0.0447\n",
      "Epoch [3/5] Batch 1600/2035                   Loss D: -0.8006, loss G: -0.0429\n",
      "Epoch [3/5] Batch 1700/2035                   Loss D: -0.4867, loss G: -0.2512\n",
      "Epoch [3/5] Batch 1800/2035                   Loss D: -0.8423, loss G: -0.0784\n",
      "Epoch [3/5] Batch 1900/2035                   Loss D: -0.6738, loss G: -0.2191\n",
      "Epoch [3/5] Batch 2000/2035                   Loss D: -0.6569, loss G: -0.0840\n",
      "Epoch [4/5] Batch 100/2035                   Loss D: -0.8370, loss G: -0.0746\n",
      "Epoch [4/5] Batch 200/2035                   Loss D: -0.6200, loss G: -0.1509\n",
      "Epoch [4/5] Batch 300/2035                   Loss D: -0.8466, loss G: -0.1245\n",
      "Epoch [4/5] Batch 400/2035                   Loss D: -0.6150, loss G: -0.1641\n",
      "Epoch [4/5] Batch 500/2035                   Loss D: -0.6567, loss G: -0.1603\n",
      "Epoch [4/5] Batch 600/2035                   Loss D: -0.9090, loss G: -0.0591\n",
      "Epoch [4/5] Batch 700/2035                   Loss D: -0.9256, loss G: -0.0422\n",
      "Epoch [4/5] Batch 800/2035                   Loss D: -0.9081, loss G: -0.0487\n",
      "Epoch [4/5] Batch 900/2035                   Loss D: -0.7885, loss G: -0.0602\n",
      "Epoch [4/5] Batch 1000/2035                   Loss D: -0.9165, loss G: -0.0501\n",
      "Epoch [4/5] Batch 1100/2035                   Loss D: -0.7012, loss G: -0.2118\n",
      "Epoch [4/5] Batch 1200/2035                   Loss D: -0.9289, loss G: -0.0252\n",
      "Epoch [4/5] Batch 1300/2035                   Loss D: -0.6848, loss G: -0.1456\n",
      "Epoch [4/5] Batch 1400/2035                   Loss D: -0.6508, loss G: -0.2315\n",
      "Epoch [4/5] Batch 1500/2035                   Loss D: -0.8024, loss G: -0.1538\n",
      "Epoch [4/5] Batch 1600/2035                   Loss D: -0.8113, loss G: -0.0697\n",
      "Epoch [4/5] Batch 1700/2035                   Loss D: -0.7280, loss G: -0.1466\n",
      "Epoch [4/5] Batch 1800/2035                   Loss D: -0.9062, loss G: -0.0460\n",
      "Epoch [4/5] Batch 1900/2035                   Loss D: -0.8344, loss G: -0.0446\n",
      "Epoch [4/5] Batch 2000/2035                   Loss D: -0.5230, loss G: -0.3353\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Hyperparameters \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 5e-5\n",
    "bs = 64\n",
    "img_size = 64\n",
    "no_channels = 3\n",
    "z_size = 100\n",
    "epochs = 5\n",
    "fc = 64\n",
    "fg = 64\n",
    "crit_iter = 5\n",
    "lamdaa = 10\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(no_channels)], [0.5 for _ in range(no_channels)]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "dataset = datasets.ImageFolder(root='Bitmoji-Faces', transform=transform)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "\n",
    "gen = Generator(z_size, no_channels, fg).to(device)\n",
    "critic = Discriminator(no_channels, fc).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(critic)\n",
    "\n",
    "\n",
    "opt_gen = optim.RMSprop(gen.parameters(), lr=lr)\n",
    "opt_critic = optim.RMSprop(critic.parameters(), lr=lr)\n",
    "\n",
    "# for tensorboard plotting\n",
    "fixed_noise = torch.randn(32, z_size, 1, 1).to(device)\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "step = 0\n",
    "\n",
    "gen.train()\n",
    "critic.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, _) in enumerate(loader):\n",
    "        data = data.to(device)\n",
    "        cur_batch_size = data.shape[0]\n",
    "\n",
    "        # max E[critic(real)] - E[critic(fake)]\n",
    "        for _ in range(crit_iter):\n",
    "            noise = torch.randn(cur_batch_size, z_size, 1, 1).to(device)\n",
    "            fake = gen(noise)\n",
    "            critic_real = critic(data).reshape(-1)\n",
    "            critic_fake = critic(fake).reshape(-1)\n",
    "            gp = gradient_penalty(critic,data,fake,device=device)\n",
    "            loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake)) + lamdaa * gp \n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            opt_critic.step()\n",
    "\n",
    "\n",
    "\n",
    "        # max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
    "        gen_fake = critic(fake).reshape(-1)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # print to tensorboard\n",
    "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "            gen.eval()\n",
    "            critic.eval()\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    data[:32], normalize=True\n",
    "                )\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:32], normalize=True\n",
    "                )\n",
    "\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "            step += 1\n",
    "            gen.train()\n",
    "            critic.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ADRL': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f09f5b5b51b597f1c6c549016ce33f987109769ac8e4eb3f83f958c2707f7a74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
