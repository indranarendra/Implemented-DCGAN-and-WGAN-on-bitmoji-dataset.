{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch import optim\n",
    "import tensorboard\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features_d, features_d * 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(features_d * 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(features_d * 4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            nn.BatchNorm2d(features_d * 8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features_d * 8, 1, 4, 2, 0),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "\n",
    "            nn.ConvTranspose2d(channels_noise, features_g * 16, 4, 1, 0),\n",
    "            nn.BatchNorm2d(features_g * 16),\n",
    "            nn.ReLU(),  \n",
    "            nn.ConvTranspose2d(features_g * 16, features_g * 8, 4, 2, 1),\n",
    "            nn.BatchNorm2d(features_g * 8),\n",
    "            nn.ReLU(),  \n",
    "            nn.ConvTranspose2d(features_g * 8, features_g * 4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(features_g * 4),\n",
    "            nn.ReLU(),  \n",
    "            nn.ConvTranspose2d(features_g * 4, features_g * 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(features_g * 2),\n",
    "            nn.ReLU(),  \n",
    "            nn.ConvTranspose2d(features_g * 2, channels_img, 4, 2, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if isinstance(m,nn.BatchNorm2d):\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5] Batch 0/1018                   Loss D: 0.6370, loss G: 3.9072\n",
      "Epoch [0/5] Batch 100/1018                   Loss D: 0.3474, loss G: 6.3488\n",
      "Epoch [0/5] Batch 200/1018                   Loss D: 0.2229, loss G: 3.6310\n",
      "Epoch [0/5] Batch 300/1018                   Loss D: 0.6767, loss G: 0.7573\n",
      "Epoch [0/5] Batch 400/1018                   Loss D: 0.4375, loss G: 1.5405\n",
      "Epoch [0/5] Batch 500/1018                   Loss D: 0.3748, loss G: 2.1263\n",
      "Epoch [0/5] Batch 600/1018                   Loss D: 0.5145, loss G: 2.2741\n",
      "Epoch [0/5] Batch 700/1018                   Loss D: 0.3502, loss G: 2.7334\n",
      "Epoch [0/5] Batch 800/1018                   Loss D: 0.3376, loss G: 3.6083\n",
      "Epoch [0/5] Batch 900/1018                   Loss D: 0.3652, loss G: 3.6951\n",
      "Epoch [0/5] Batch 1000/1018                   Loss D: 0.7770, loss G: 4.9429\n",
      "Epoch [1/5] Batch 0/1018                   Loss D: 0.3682, loss G: 1.7557\n",
      "Epoch [1/5] Batch 100/1018                   Loss D: 0.3116, loss G: 3.8016\n",
      "Epoch [1/5] Batch 200/1018                   Loss D: 0.2982, loss G: 3.3051\n",
      "Epoch [1/5] Batch 300/1018                   Loss D: 0.1806, loss G: 2.3096\n",
      "Epoch [1/5] Batch 400/1018                   Loss D: 0.1903, loss G: 2.9336\n",
      "Epoch [1/5] Batch 500/1018                   Loss D: 0.2988, loss G: 1.5918\n",
      "Epoch [1/5] Batch 600/1018                   Loss D: 0.3112, loss G: 1.3031\n",
      "Epoch [1/5] Batch 700/1018                   Loss D: 0.2454, loss G: 1.0623\n",
      "Epoch [1/5] Batch 800/1018                   Loss D: 0.1628, loss G: 3.4347\n",
      "Epoch [1/5] Batch 900/1018                   Loss D: 0.1504, loss G: 2.6335\n",
      "Epoch [1/5] Batch 1000/1018                   Loss D: 0.3017, loss G: 1.1955\n",
      "Epoch [2/5] Batch 0/1018                   Loss D: 0.1849, loss G: 3.7211\n",
      "Epoch [2/5] Batch 100/1018                   Loss D: 0.1106, loss G: 3.8645\n",
      "Epoch [2/5] Batch 200/1018                   Loss D: 0.1341, loss G: 2.4044\n",
      "Epoch [2/5] Batch 300/1018                   Loss D: 0.3876, loss G: 0.8674\n",
      "Epoch [2/5] Batch 400/1018                   Loss D: 0.1101, loss G: 3.4717\n",
      "Epoch [2/5] Batch 500/1018                   Loss D: 0.0886, loss G: 3.1974\n",
      "Epoch [2/5] Batch 600/1018                   Loss D: 0.4014, loss G: 4.1285\n",
      "Epoch [2/5] Batch 700/1018                   Loss D: 0.1178, loss G: 3.7971\n",
      "Epoch [2/5] Batch 800/1018                   Loss D: 0.0801, loss G: 3.4377\n",
      "Epoch [2/5] Batch 900/1018                   Loss D: 0.1195, loss G: 3.5443\n",
      "Epoch [2/5] Batch 1000/1018                   Loss D: 0.0586, loss G: 3.6331\n",
      "Epoch [3/5] Batch 0/1018                   Loss D: 0.0605, loss G: 3.8417\n",
      "Epoch [3/5] Batch 100/1018                   Loss D: 0.6067, loss G: 13.0551\n",
      "Epoch [3/5] Batch 200/1018                   Loss D: 0.1583, loss G: 4.1856\n",
      "Epoch [3/5] Batch 300/1018                   Loss D: 0.0742, loss G: 3.4695\n",
      "Epoch [3/5] Batch 400/1018                   Loss D: 0.1167, loss G: 3.9382\n",
      "Epoch [3/5] Batch 500/1018                   Loss D: 0.1038, loss G: 3.9224\n",
      "Epoch [3/5] Batch 600/1018                   Loss D: 0.0945, loss G: 3.2716\n",
      "Epoch [3/5] Batch 700/1018                   Loss D: 0.0552, loss G: 3.3287\n",
      "Epoch [3/5] Batch 800/1018                   Loss D: 0.0322, loss G: 4.4799\n",
      "Epoch [3/5] Batch 900/1018                   Loss D: 0.0987, loss G: 4.1017\n",
      "Epoch [3/5] Batch 1000/1018                   Loss D: 0.0853, loss G: 2.8725\n",
      "Epoch [4/5] Batch 0/1018                   Loss D: 0.0688, loss G: 3.7736\n",
      "Epoch [4/5] Batch 100/1018                   Loss D: 0.0733, loss G: 2.7493\n",
      "Epoch [4/5] Batch 200/1018                   Loss D: 0.2313, loss G: 3.2015\n",
      "Epoch [4/5] Batch 300/1018                   Loss D: 0.2199, loss G: 6.6516\n",
      "Epoch [4/5] Batch 400/1018                   Loss D: 0.0861, loss G: 4.0672\n",
      "Epoch [4/5] Batch 500/1018                   Loss D: 0.0712, loss G: 4.1603\n",
      "Epoch [4/5] Batch 600/1018                   Loss D: 0.0502, loss G: 4.7535\n",
      "Epoch [4/5] Batch 700/1018                   Loss D: 0.0382, loss G: 3.7696\n",
      "Epoch [4/5] Batch 800/1018                   Loss D: 0.0299, loss G: 4.4408\n",
      "Epoch [4/5] Batch 900/1018                   Loss D: 0.0580, loss G: 5.1798\n",
      "Epoch [4/5] Batch 1000/1018                   Loss D: 0.0702, loss G: 3.7068\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 2e-4\n",
    "bs = 128\n",
    "img_size = 64\n",
    "no_channels = 3\n",
    "noise_size = 100\n",
    "epochs = 5\n",
    "fd = 64\n",
    "fg = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(no_channels)], [0.5 for _ in range(no_channels)]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "dataset = datasets.ImageFolder(root='Bitmoji-Faces', transform=transform)\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "gen = Generator(noise_size, no_channels, fg).to(device)\n",
    "disc = Discriminator(no_channels, fd).to(device)\n",
    "\n",
    "initialize_weights(gen)\n",
    "initialize_weights(disc)\n",
    "\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# for tensorboard plotting\n",
    "fixed_noise = torch.randn(32, noise_size, 1, 1).to(device)\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "step = 0\n",
    "\n",
    "gen.train()\n",
    "disc.train()\n",
    "\n",
    "const_z = torch.randn(bs, noise_size, 1, 1).to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (real,_) in enumerate(dataloader):\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(real.shape[0], noise_size, 1, 1).to(device)\n",
    "        fake = gen(noise)\n",
    "\n",
    "        # max log(D(x)) + log(1 - D(G(z)))\n",
    "        disc_real = disc(real).reshape(-1)\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake.detach()).reshape(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        # min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        output = disc(fake).reshape(-1)\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # print to tensorboard\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{epochs}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(const_z)\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real[:32], normalize=True\n",
    "                )\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:32], normalize=True\n",
    "                )\n",
    "\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "906876fe13a5738ee4b80b56eccc5d13a8a6a260ae5d53d00dfdc5d20753f739"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
